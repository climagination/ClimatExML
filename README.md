# ClimatExML

## MLFlow Configuration on the Alliance

Alliance provides persistant storage on arbutus which is accessible here https://arbutus.cloud.computecanada.ca/. You can also make object stores here.

MLFlow artifacts can be logged to an instance running an mlflow server. This server must have `mlflow` installed, as well as `boto3`. (Side note, you have to manually add other ssh machines and can't do it from openstack).

Use a config file to set up the MLFlow environment variables from your training environment:
```bash
export MLFLOW_TRACKING_URI='<public-ip-of-instance-on-arbutus>:5000'
export MLFLOW_S3_ENDPOINT_URL='https://object-arbutus.cloud.computecanada.ca/'
export AWS_ACCESS_KEY_ID="<get-this-from-openstack>"
export AWS_SECRET_ACCESS_KEY="<get-this-from-openstack>"
```

To get the keys from openstack you must first download the `...opensrc.sh` file from the openstack client. `...` corresponds to your specific allcoation and isntance names automatically generated by openstack.

`source ...openrc.sh` 
`openstack ec2 credentials create`

This will create a file and you want the `access` key to be set as `AWS_ACCESS_KEY_ID` and the `secret` key to be set as `AWS_SECTRET_ACCESS_KEY` from the generated info. `source` the mlflow environment bash to set the variables.

Note, you will want to use the same environment variables on the server running your mlflow server instance. 

Then you can spin up your mlflow server remotely at:
`mlflow server --backend-store-uri postgresql://<username>:<password>@localhost:5432/mlflowdb --host 0.0.0.0 --default-artifact-root s3://<name_of_bucket>`

You will have to configure a postgresql database named `mlflowdb` and set a user and password beforehand. 

Also note that `<name_of_bucket>` is the name of the object store you create in openstack. 

## Useful Apptainer Commands for the Alliance Machines
